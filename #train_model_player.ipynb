{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wB8ssWoIuuY5",
    "outputId": "46628a52-0887-4634-8805-b74269fcc1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully loaded in 00m06s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import _to_tensor\n",
    "\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.callbacks import LearningRateScheduler, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import (Layer, Input, Embedding, Dropout, Dense,\n",
    "                          TimeDistributed, concatenate, BatchNormalization,\n",
    "                          Reshape, Flatten, GlobalAveragePooling1D, add,\n",
    "                          Lambda, subtract, Bidirectional, GRU)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "df = pd.read_csv('full_dataset.csv')\n",
    "print(\"Fully loaded in {:02d}m{:02d}s\".format(*divmod(int(time.time() - t0), 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "l64uBsPCuyZP",
    "outputId": "eba8e8ea-8d48-4b5a-ab8a-bf19903322a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words :  1186\n",
      "not_found: <END_PERIOD_1>\n",
      "not_found: <START_PERIOD_2>\n",
      "not_found: <END_PERIOD_2>\n",
      "not_found: <START_PERIOD_1>\n",
      "Embedding mat :  (1187, 200)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = \"./embeddings/_event_vectors_200dim_15epochs.txt\"\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n",
    "    \n",
    "\n",
    "d = defaultdict(int)\n",
    "\n",
    "x = df.action_SEQ.str.split(' / ')\n",
    "for elt in x:\n",
    "    for w in elt:\n",
    "        d[w] += 1\n",
    "        \n",
    "x = df.before_SEQ.str.split(' / ')\n",
    "for elt in x:\n",
    "    for sents in elt:\n",
    "        words = sents.split()\n",
    "        for w in words:\n",
    "            if w not in d:\n",
    "                d[w] += 1\n",
    "            \n",
    "x = df.after_SEQ.str.split(' / ')\n",
    "for elt in x:\n",
    "    for sents in elt:\n",
    "        words = sents.split()\n",
    "        for w in words:\n",
    "            if w not in d:\n",
    "                d[w] += 1\n",
    "\n",
    "d = dict(d)\n",
    "\n",
    "all_words = set(d.keys())\n",
    "print('All words : ', len(all_words))\n",
    "\n",
    "embeddings_index = get_embedding()\n",
    "\n",
    "docs = sum([[key] * value for key, value in d.items()], [])\n",
    "shuffle(docs)\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters=\"\")\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "rv_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print('not_found: %s' % word)\n",
    "        \n",
    "print('Embedding mat : ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DCgdIMJzvAIm"
   },
   "outputs": [],
   "source": [
    "with open('./maps/player2mins.json', 'r', encoding='utf8') as fp:\n",
    "    player2mins = json.load(fp)\n",
    "\n",
    "df = df[df.player.isin(player2mins.keys())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Isn1pgd5umba",
    "outputId": "8b408797-3e5c-43a2-c63a-8e34d57e7e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37380, 897)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WddWVbsHvHsN"
   },
   "outputs": [],
   "source": [
    "def index_and_pad(seq, wmap = word_index, value = 0, maxlen = 5, reverse = False):\n",
    "    if wmap is not None:\n",
    "        seq = [[word_index[w] for w in s.split()] for s in seq]\n",
    "    else:\n",
    "        seq = [[int(w) for w in s.split()] for s in seq]\n",
    "    if reverse:\n",
    "        seq = [[value] * (maxlen - len(s)) + s if len(s)<maxlen else s[:maxlen] for s in seq]\n",
    "    else:\n",
    "        seq = [s + [value] * (maxlen - len(s)) if len(s)<maxlen else s[:maxlen] for s in seq]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "B3ZYHh1fvSpb",
    "outputId": "f25c3fbb-9309-4943-c569-301898dc8b1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37380, 295), (37380, 295), (37380, 284))"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap = dict(enumerate(pd.get_dummies(df.player)))\n",
    "rv_cmap = {v: k for k, v in cmap.items()}\n",
    "\n",
    "Y = pd.get_dummies(df.player).values\n",
    "\n",
    "X_HOME = df[[c for c in df.columns if c.endswith('_HOME')]].values\n",
    "X_AWAY = df[[c for c in df.columns if c.endswith('_AWAY')]].values\n",
    "X_PLAYER = df[[c for c in df.columns if c.endswith('_PLAYER')]].values\n",
    "\n",
    "X_HOME.shape, X_AWAY.shape, X_PLAYER.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XMpdYgQrvUQJ"
   },
   "outputs": [],
   "source": [
    "n_features_team = X_HOME.shape[1]\n",
    "n_features_player = X_PLAYER.shape[1]\n",
    "n_cats = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PTYPSgoavVku"
   },
   "outputs": [],
   "source": [
    "n_units = 50\n",
    "drop_rate = 0.5\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self, return_coefficients=False,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        \n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), a]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]\n",
    "          \n",
    "    def get_config(self):\n",
    "        config = {\"return_coefficients\": self.return_coefficients,\n",
    "       \"W_regularizer\": self.W_regularizer,\n",
    "       \"u_regularizer\": self.u_regularizer,\n",
    "       \"b_regularizer\": self.b_regularizer,\n",
    "       \"W_constraint\": self.W_constraint,\n",
    "       \"u_constraint\": self.u_constraint,\n",
    "       \"b_constraint\": self.b_constraint,\n",
    "       \"bias\": self.bias}\n",
    "        base_config = super(AttentionWithContext, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SplitLayer(Layer):\n",
    "\n",
    "    def __init__(self, range_, **kwargs):\n",
    "        self.range_ = range_\n",
    "        self.start = self.range_[0]\n",
    "        self.end = self.range_[1]\n",
    "        self.output_dim = self.end - self.start\n",
    "        super(SplitLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SplitLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x[:, self.start:self.end]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "      \n",
    "    def get_config(self):\n",
    "        config = {'range_': self.range_}\n",
    "        base_config = super(SplitLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "37SPn_jqvYvy"
   },
   "outputs": [],
   "source": [
    "MAX_I = 30\n",
    "\n",
    "def get_action_encoder():\n",
    "    raw_input = Input(shape=(2*1 +4*5,))\n",
    "            \n",
    "    target_input = SplitLayer((0, 1))(raw_input)\n",
    "    target_teams_input = SplitLayer((1, 2))(raw_input)\n",
    "\n",
    "    bf_ctx_input = SplitLayer((2, 7))(raw_input)\n",
    "    bf_teams_input = SplitLayer((7, 12))(raw_input)\n",
    "\n",
    "    af_ctx_input = SplitLayer((12, 17))(raw_input)\n",
    "    af_teams_input = SplitLayer((17, 22))(raw_input)\n",
    "    \n",
    "    embed_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=None,  # sentence size vary from batch to batch\n",
    "                            trainable=False\n",
    "                            )\n",
    "\n",
    "    bf_ctx_wv = embed_layer(bf_ctx_input)\n",
    "    af_ctx_wv = embed_layer(af_ctx_input)\n",
    "    target_wv = embed_layer(target_input)\n",
    "\n",
    "    bf_ctx_wv = concatenate([bf_ctx_wv, Reshape((5, 1))(bf_teams_input)], axis=-1)\n",
    "    af_ctx_wv = concatenate([af_ctx_wv, Reshape((5, 1))(af_teams_input)], axis=-1)\n",
    "    target_wv = concatenate([target_wv, Reshape((1, 1))(target_teams_input)], axis=-1)\n",
    "\n",
    "    bidir_gru = Bidirectional(GRU(units=n_units,\n",
    "                                     activation='tanh', \n",
    "                                     dropout=0.0,\n",
    "                                     recurrent_dropout=0.0,\n",
    "                                     implementation=1,\n",
    "                                     return_sequences=True,\n",
    "                                     reset_after=True,\n",
    "                                     recurrent_activation='sigmoid'),\n",
    "                                 merge_mode='concat', weights=None)\n",
    "\n",
    "\n",
    "    sent_bf_ctx = bidir_gru(bf_ctx_wv)\n",
    "    sent_af_ctx = bidir_gru(af_ctx_wv)\n",
    "\n",
    "    sent_bf_vec = AttentionWithContext(return_coefficients=False)(\n",
    "            sent_bf_ctx)\n",
    "    sent_bf_vec_dr = Dropout(drop_rate)(sent_bf_vec)\n",
    "\n",
    "    sent_af_vec = AttentionWithContext(return_coefficients=False)(\n",
    "            sent_af_ctx)\n",
    "    sent_af_vec_dr = Dropout(drop_rate)(sent_af_vec)   \n",
    "\n",
    "    target_vec = bidir_gru(target_wv)\n",
    "    target_vec = Flatten()(target_vec)\n",
    "    target_vec_dr = Dropout(drop_rate)(target_vec)\n",
    "\n",
    "    out = concatenate([target_vec_dr, sent_bf_vec_dr, sent_af_vec_dr], axis=-1)\n",
    "    out = Reshape((3, 2 * n_units))(out)\n",
    "\n",
    "    bidir_gru_2 = Bidirectional(GRU(units=n_units,\n",
    "                                     activation='tanh', \n",
    "                                     dropout=0.0,\n",
    "                                     recurrent_dropout=0.0,\n",
    "                                     implementation=1,\n",
    "                                     return_sequences=False,\n",
    "                                     reset_after=True,\n",
    "                                     recurrent_activation='sigmoid'),\n",
    "                                 merge_mode='concat', weights=None)\n",
    "\n",
    "    out = bidir_gru_2(out)\n",
    "    out = Dropout(drop_rate)(out)\n",
    "    \n",
    "    out = Dense(256, activation='relu')(out)\n",
    "\n",
    "    model = Model(raw_input, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ew24ahY7vdS5"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    home_input = Input(shape=(n_features_team,))\n",
    "    away_input = Input(shape=(n_features_team,))\n",
    "    player_input = Input(shape=(n_features_player,))\n",
    "\n",
    "    target_input = Input(shape=(MAX_I, 1,))\n",
    "    target_teams_input = Input(shape=(MAX_I, 1,))\n",
    "\n",
    "    bf_ctx_input = Input(shape=(MAX_I, 5,))\n",
    "    bf_teams_input = Input(shape=(MAX_I, 5,))\n",
    "\n",
    "    af_ctx_input = Input(shape=(MAX_I, 5,))\n",
    "    af_teams_input = Input(shape=(MAX_I, 5,))\n",
    "    \n",
    "    timings_input = Input(shape = (12,))\n",
    "\n",
    "    home_dense = BatchNormalization()(home_input)\n",
    "    home_dense = Dense(64, activation='relu')(home_dense)##\n",
    "    home_dense = Dropout(drop_rate)(home_dense)\n",
    "\n",
    "    away_dense = BatchNormalization()(away_input)\n",
    "    away_dense = Dense(64, activation='relu')(away_dense)##\n",
    "    away_dense = Dropout(drop_rate)(away_dense)\n",
    "\n",
    "    inter = subtract([home_dense, away_dense])\n",
    "\n",
    "    inter = Dense(64, activation='relu')(##\n",
    "        concatenate([home_dense, away_dense, inter]))\n",
    "    inter = Dropout(drop_rate)(inter)\n",
    "\n",
    "    home_dense = Dense(128, activation='relu')(\n",
    "        concatenate([home_dense, inter]))\n",
    "\n",
    "    away_dense = Dense(128, activation='relu')(\n",
    "        concatenate([away_dense, inter]))\n",
    "\n",
    "    player_dense = BatchNormalization()(player_input)\n",
    "    player_dense = Dense(128, activation='relu')(player_dense)\n",
    "    player_dense = Dropout(drop_rate)(player_dense)\n",
    "\n",
    "    action_input = concatenate([target_input, target_teams_input,\n",
    "                                bf_ctx_input, bf_teams_input, af_ctx_input, af_teams_input])\n",
    "\n",
    "    action_encoder = get_action_encoder()\n",
    "    actions_encoded = TimeDistributed(action_encoder)(action_input)\n",
    "    actions_encoded = GlobalAveragePooling1D()(actions_encoded)\n",
    "\n",
    "    home_pred = Dense(20, activation=\"softmax\", name=\"home\")(home_dense)\n",
    "\n",
    "    away_pred = Dense(20, activation='softmax', name=\"away\")(away_dense)\n",
    "\n",
    "    frag_encoded = concatenate([player_dense, home_dense, away_dense, actions_encoded, timings_input])\n",
    "\n",
    "    frag_encoded = Dense(512, activation='relu')(frag_encoded)\n",
    "    frag_encoded = Dropout(drop_rate)(frag_encoded)\n",
    "\n",
    "    player_pred = Dense(n_cats, activation='softmax', name=\"player\")(frag_encoded)\n",
    "\n",
    "    model=Model([home_input, away_input, player_input, target_input, target_teams_input,\n",
    "                   bf_ctx_input, bf_teams_input, af_ctx_input, af_teams_input, timings_input], [home_pred, away_pred, player_pred])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oD0Qh1Eyve8l"
   },
   "outputs": [],
   "source": [
    "X_BEFORE_CTXS = df.before_SEQ.str.split(' / ').apply(index_and_pad, **{\"reverse\": True})\n",
    "X_BEFORE_CTXS = np.array(X_BEFORE_CTXS)\n",
    "\n",
    "X_AFTER_CTXS = df.after_SEQ.str.split(' / ').apply(index_and_pad)\n",
    "X_AFTER_CTXS = np.array(X_AFTER_CTXS)\n",
    "\n",
    "\n",
    "X_BEFORE_TEAMS = df.teams_before_SEQ.str.split(' / ').apply(index_and_pad, **{\"value\": -1, \"wmap\": None, \"reverse\": True})\n",
    "X_BEFORE_TEAMS = np.array(X_BEFORE_TEAMS)\n",
    "\n",
    "X_AFTER_TEAMS = df.teams_after_SEQ.str.split(' / ').apply(index_and_pad, **{\"value\": -1, \"wmap\": None})\n",
    "X_AFTER_TEAMS = np.array(X_AFTER_TEAMS)\n",
    "\n",
    "X_ACTIONS = df.action_SEQ.str.split(' / ').apply(index_and_pad, **{\"maxlen\": 1})\n",
    "X_ACTIONS = np.array([sum(s,[]) for s in X_ACTIONS])\n",
    "X_ACTIONS = np.array([s + [0] * (MAX_I - len(s)) if len(s)\n",
    "                      < MAX_I else s[:MAX_I] for s in X_ACTIONS])\n",
    "\n",
    "X_ACTIONS = X_ACTIONS[..., np.newaxis]\n",
    "X_ACTIONS_TEAMS = np.ones(X_ACTIONS.shape).astype(int)\n",
    "\n",
    "X_BEFORE_CTXS = np.array([np.pad(s, (0,MAX_I-len(s)), \"constant\", constant_values = 0)[:,:5] if len(s)\n",
    "                      < MAX_I else s[:MAX_I] for s in X_BEFORE_CTXS])\n",
    "\n",
    "X_AFTER_CTXS = np.array([np.pad(s, (0,MAX_I-len(s)), \"constant\", constant_values = 0)[:,:5] if len(s)\n",
    "                      < MAX_I else s[:MAX_I] for s in X_AFTER_CTXS])\n",
    "\n",
    "X_BEFORE_TEAMS = np.array([np.pad(s, (0, MAX_I - len(s)), \"constant\", constant_values=-1)[:, :5] if len(s)\n",
    "                           < MAX_I else s[:MAX_I] for s in X_BEFORE_TEAMS])\n",
    "\n",
    "X_AFTER_TEAMS = np.array([np.pad(s, (0, MAX_I - len(s)), \"constant\", constant_values=-1)[:, :5] if len(s)\n",
    "                           < MAX_I else s[:MAX_I] for s in X_AFTER_TEAMS])\n",
    "X_TIMINGS = df[[c for c in df.columns if c.startswith('TIMING')]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r5e5B1Tz5P2a"
   },
   "outputs": [],
   "source": [
    "y_home = pd.get_dummies(df.team_home).values\n",
    "y_away = pd.get_dummies(df.team_away).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1492
    },
    "colab_type": "code",
    "id": "u5rIux788zYf",
    "outputId": "b3b6bbf0-e054-4ff5-e834-6da960e782c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 1\n",
      "embedding_1 True\n",
      "Train on 29809 samples, validate on 7571 samples\n",
      "Epoch 1/6\n",
      "29809/29809 [==============================] - 25s 836us/step - loss: 276.0258 - home_loss: 3.1712 - away_loss: 3.1764 - player_loss: 5.3936 - home_acc: 0.0621 - away_acc: 0.0585 - player_acc: 0.0088 - val_loss: 261.3824 - val_home_loss: 2.9317 - val_away_loss: 2.9491 - val_player_loss: 5.1100 - val_home_acc: 0.1150 - val_away_acc: 0.0992 - val_player_acc: 0.0229\n",
      "Epoch 2/6\n",
      "29809/29809 [==============================] - 19s 635us/step - loss: 250.2520 - home_loss: 2.9622 - away_loss: 2.9664 - player_loss: 4.8865 - home_acc: 0.0911 - away_acc: 0.0878 - player_acc: 0.0317 - val_loss: 226.2907 - val_home_loss: 2.8616 - val_away_loss: 2.8629 - val_player_loss: 4.4113 - val_home_acc: 0.1679 - val_away_acc: 0.1469 - val_player_acc: 0.0687\n",
      "Epoch 3/6\n",
      "29809/29809 [==============================] - 19s 637us/step - loss: 227.1500 - home_loss: 2.8667 - away_loss: 2.8589 - player_loss: 4.4285 - home_acc: 0.1218 - away_acc: 0.1201 - player_acc: 0.0504 - val_loss: 207.5863 - val_home_loss: 2.7583 - val_away_loss: 2.7524 - val_player_loss: 4.0415 - val_home_acc: 0.1807 - val_away_acc: 0.1936 - val_player_acc: 0.0950\n",
      "Epoch 4/6\n",
      "29809/29809 [==============================] - 20s 661us/step - loss: 214.1435 - home_loss: 2.7610 - away_loss: 2.7457 - player_loss: 4.1727 - home_acc: 0.1520 - away_acc: 0.1526 - player_acc: 0.0699 - val_loss: 198.1669 - val_home_loss: 2.6439 - val_away_loss: 2.6248 - val_player_loss: 3.8580 - val_home_acc: 0.2314 - val_away_acc: 0.2158 - val_player_acc: 0.1141\n",
      "Epoch 5/6\n",
      "29809/29809 [==============================] - 19s 642us/step - loss: 205.9094 - home_loss: 2.6995 - away_loss: 2.6783 - player_loss: 4.0106 - home_acc: 0.1727 - away_acc: 0.1715 - player_acc: 0.0831 - val_loss: 193.9989 - val_home_loss: 2.6099 - val_away_loss: 2.5954 - val_player_loss: 3.7759 - val_home_acc: 0.2339 - val_away_acc: 0.2285 - val_player_acc: 0.1280\n",
      "Epoch 6/6\n",
      "29809/29809 [==============================] - 19s 640us/step - loss: 203.5009 - home_loss: 2.6733 - away_loss: 2.6523 - player_loss: 3.9635 - home_acc: 0.1750 - away_acc: 0.1801 - player_acc: 0.0902 - val_loss: 192.1170 - val_home_loss: 2.5780 - val_away_loss: 2.5694 - val_player_loss: 3.7394 - val_home_acc: 0.2383 - val_away_acc: 0.2414 - val_player_acc: 0.1356\n",
      "Saved model to disk\n",
      "MODEL: 2\n",
      "embedding_1 True\n",
      "Train on 29857 samples, validate on 7523 samples\n",
      "Epoch 1/6\n",
      "29857/29857 [==============================] - 25s 852us/step - loss: 274.7979 - home_loss: 3.1642 - away_loss: 3.1673 - player_loss: 5.3693 - home_acc: 0.0605 - away_acc: 0.0602 - player_acc: 0.0111 - val_loss: 257.3442 - val_home_loss: 2.9613 - val_away_loss: 2.9435 - val_player_loss: 5.0288 - val_home_acc: 0.0767 - val_away_acc: 0.0962 - val_player_acc: 0.0322\n",
      "Epoch 2/6\n",
      "29857/29857 [==============================] - 19s 647us/step - loss: 247.9771 - home_loss: 2.9790 - away_loss: 2.9692 - player_loss: 4.8406 - home_acc: 0.0828 - away_acc: 0.0814 - player_acc: 0.0310 - val_loss: 224.0280 - val_home_loss: 2.9018 - val_away_loss: 2.8771 - val_player_loss: 4.3650 - val_home_acc: 0.1273 - val_away_acc: 0.1357 - val_player_acc: 0.0716\n",
      "Epoch 3/6\n",
      "29857/29857 [==============================] - 19s 645us/step - loss: 225.9504 - home_loss: 2.8915 - away_loss: 2.8671 - player_loss: 4.4038 - home_acc: 0.1164 - away_acc: 0.1128 - player_acc: 0.0519 - val_loss: 206.8605 - val_home_loss: 2.8302 - val_away_loss: 2.7690 - val_player_loss: 4.0252 - val_home_acc: 0.1787 - val_away_acc: 0.1648 - val_player_acc: 0.0970\n",
      "Epoch 4/6\n",
      "29857/29857 [==============================] - 19s 645us/step - loss: 213.2238 - home_loss: 2.7983 - away_loss: 2.7600 - player_loss: 4.1533 - home_acc: 0.1459 - away_acc: 0.1467 - player_acc: 0.0698 - val_loss: 196.1034 - val_home_loss: 2.7061 - val_away_loss: 2.6480 - val_player_loss: 3.8150 - val_home_acc: 0.2225 - val_away_acc: 0.2110 - val_player_acc: 0.1244\n",
      "Epoch 5/6\n",
      "29857/29857 [==============================] - 20s 674us/step - loss: 205.3694 - home_loss: 2.7261 - away_loss: 2.6963 - player_loss: 3.9989 - home_acc: 0.1733 - away_acc: 0.1671 - player_acc: 0.0847 - val_loss: 192.8729 - val_home_loss: 2.6662 - val_away_loss: 2.6167 - val_player_loss: 3.7518 - val_home_acc: 0.2309 - val_away_acc: 0.2242 - val_player_acc: 0.1352\n",
      "Epoch 6/6\n",
      "29857/29857 [==============================] - 20s 654us/step - loss: 203.5316 - home_loss: 2.6951 - away_loss: 2.6731 - player_loss: 3.9633 - home_acc: 0.1764 - away_acc: 0.1741 - player_acc: 0.0882 - val_loss: 191.2974 - val_home_loss: 2.6290 - val_away_loss: 2.5901 - val_player_loss: 3.7216 - val_home_acc: 0.2411 - val_away_acc: 0.2339 - val_player_acc: 0.1416\n",
      "Saved model to disk\n",
      "MODEL: 3\n",
      "embedding_1 True\n",
      "Train on 29899 samples, validate on 7481 samples\n",
      "Epoch 1/6\n",
      "29899/29899 [==============================] - 26s 883us/step - loss: 274.6968 - home_loss: 3.1637 - away_loss: 3.1730 - player_loss: 5.3672 - home_acc: 0.0596 - away_acc: 0.0587 - player_acc: 0.0117 - val_loss: 256.5442 - val_home_loss: 2.9331 - val_away_loss: 2.9392 - val_player_loss: 5.0134 - val_home_acc: 0.1024 - val_away_acc: 0.0937 - val_player_acc: 0.0348\n",
      "Epoch 2/6\n",
      "29899/29899 [==============================] - 20s 660us/step - loss: 246.6264 - home_loss: 2.9650 - away_loss: 2.9727 - player_loss: 4.8138 - home_acc: 0.0914 - away_acc: 0.0846 - player_acc: 0.0313 - val_loss: 222.6222 - val_home_loss: 2.8656 - val_away_loss: 2.8808 - val_player_loss: 4.3375 - val_home_acc: 0.1591 - val_away_acc: 0.1437 - val_player_acc: 0.0679\n",
      "Epoch 3/6\n",
      "29899/29899 [==============================] - 20s 658us/step - loss: 224.8964 - home_loss: 2.8665 - away_loss: 2.8778 - player_loss: 4.3830 - home_acc: 0.1220 - away_acc: 0.1090 - player_acc: 0.0541 - val_loss: 206.0900 - val_home_loss: 2.7554 - val_away_loss: 2.7895 - val_player_loss: 4.0109 - val_home_acc: 0.2004 - val_away_acc: 0.1905 - val_player_acc: 0.0961\n",
      "Epoch 4/6\n",
      "29899/29899 [==============================] - 19s 648us/step - loss: 212.8574 - home_loss: 2.7482 - away_loss: 2.7678 - player_loss: 4.1468 - home_acc: 0.1597 - away_acc: 0.1418 - player_acc: 0.0706 - val_loss: 197.0531 - val_home_loss: 2.6322 - val_away_loss: 2.6590 - val_player_loss: 3.8352 - val_home_acc: 0.2365 - val_away_acc: 0.2250 - val_player_acc: 0.1113\n",
      "Epoch 5/6\n",
      "29899/29899 [==============================] - 19s 647us/step - loss: 205.6854 - home_loss: 2.6817 - away_loss: 2.6906 - player_loss: 4.0063 - home_acc: 0.1822 - away_acc: 0.1692 - player_acc: 0.0830 - val_loss: 193.4810 - val_home_loss: 2.5967 - val_away_loss: 2.6216 - val_player_loss: 3.7653 - val_home_acc: 0.2466 - val_away_acc: 0.2363 - val_player_acc: 0.1204\n",
      "Epoch 6/6\n",
      "29899/29899 [==============================] - 20s 656us/step - loss: 203.3958 - home_loss: 2.6542 - away_loss: 2.6632 - player_loss: 3.9616 - home_acc: 0.1876 - away_acc: 0.1739 - player_acc: 0.0905 - val_loss: 191.8160 - val_home_loss: 2.5615 - val_away_loss: 2.5868 - val_player_loss: 3.7334 - val_home_acc: 0.2601 - val_away_acc: 0.2437 - val_player_acc: 0.1291\n",
      "Saved model to disk\n",
      "MODEL: 4\n",
      "embedding_1 True\n",
      "Train on 29952 samples, validate on 7428 samples\n",
      "Epoch 1/6\n",
      "29952/29952 [==============================] - 26s 868us/step - loss: 275.6148 - home_loss: 3.1598 - away_loss: 3.1410 - player_loss: 5.3863 - home_acc: 0.0637 - away_acc: 0.0612 - player_acc: 0.0094 - val_loss: 258.9310 - val_home_loss: 2.9340 - val_away_loss: 2.9229 - val_player_loss: 5.0615 - val_home_acc: 0.0994 - val_away_acc: 0.1074 - val_player_acc: 0.0258\n",
      "Epoch 2/6\n",
      "29952/29952 [==============================] - 19s 649us/step - loss: 248.3659 - home_loss: 2.9693 - away_loss: 2.9674 - player_loss: 4.8486 - home_acc: 0.0875 - away_acc: 0.0844 - player_acc: 0.0303 - val_loss: 222.3498 - val_home_loss: 2.8825 - val_away_loss: 2.8804 - val_player_loss: 4.3317 - val_home_acc: 0.1498 - val_away_acc: 0.1559 - val_player_acc: 0.0691\n",
      "Epoch 3/6\n",
      "29952/29952 [==============================] - 19s 650us/step - loss: 226.0859 - home_loss: 2.8782 - away_loss: 2.8855 - player_loss: 4.4064 - home_acc: 0.1192 - away_acc: 0.1161 - player_acc: 0.0521 - val_loss: 206.2857 - val_home_loss: 2.7808 - val_away_loss: 2.7966 - val_player_loss: 4.0142 - val_home_acc: 0.1910 - val_away_acc: 0.1883 - val_player_acc: 0.0952\n",
      "Epoch 4/6\n",
      "29952/29952 [==============================] - 20s 666us/step - loss: 214.1398 - home_loss: 2.7630 - away_loss: 2.7824 - player_loss: 4.1719 - home_acc: 0.1518 - away_acc: 0.1470 - player_acc: 0.0660 - val_loss: 196.8356 - val_home_loss: 2.6526 - val_away_loss: 2.6580 - val_player_loss: 3.8305 - val_home_acc: 0.2357 - val_away_acc: 0.2231 - val_player_acc: 0.1214\n",
      "Epoch 5/6\n",
      "29952/29952 [==============================] - 19s 649us/step - loss: 205.8887 - home_loss: 2.6980 - away_loss: 2.7061 - player_loss: 4.0097 - home_acc: 0.1720 - away_acc: 0.1694 - player_acc: 0.0825 - val_loss: 193.1461 - val_home_loss: 2.6165 - val_away_loss: 2.6228 - val_player_loss: 3.7581 - val_home_acc: 0.2501 - val_away_acc: 0.2340 - val_player_acc: 0.1313\n",
      "Epoch 6/6\n",
      "29952/29952 [==============================] - 19s 650us/step - loss: 203.6351 - home_loss: 2.6699 - away_loss: 2.6811 - player_loss: 3.9657 - home_acc: 0.1814 - away_acc: 0.1765 - player_acc: 0.0881 - val_loss: 191.1887 - val_home_loss: 2.5745 - val_away_loss: 2.5866 - val_player_loss: 3.7206 - val_home_acc: 0.2614 - val_away_acc: 0.2589 - val_player_acc: 0.1384\n",
      "Saved model to disk\n",
      "MODEL: 5\n",
      "embedding_1 True\n",
      "Train on 30003 samples, validate on 7377 samples\n",
      "Epoch 1/6\n",
      "30003/30003 [==============================] - 27s 892us/step - loss: 275.5572 - home_loss: 3.1715 - away_loss: 3.1362 - player_loss: 5.3850 - home_acc: 0.0603 - away_acc: 0.0615 - player_acc: 0.0113 - val_loss: 258.4742 - val_home_loss: 2.9392 - val_away_loss: 2.9289 - val_player_loss: 5.0521 - val_home_acc: 0.0903 - val_away_acc: 0.1019 - val_player_acc: 0.0275\n",
      "Epoch 2/6\n",
      "30003/30003 [==============================] - 19s 649us/step - loss: 248.9450 - home_loss: 2.9723 - away_loss: 2.9665 - player_loss: 4.8601 - home_acc: 0.0821 - away_acc: 0.0854 - player_acc: 0.0300 - val_loss: 226.4472 - val_home_loss: 2.8839 - val_away_loss: 2.8734 - val_player_loss: 4.4138 - val_home_acc: 0.1377 - val_away_acc: 0.1433 - val_player_acc: 0.0583\n",
      "Epoch 3/6\n",
      "30003/30003 [==============================] - 20s 673us/step - loss: 227.0120 - home_loss: 2.8874 - away_loss: 2.8698 - player_loss: 4.4251 - home_acc: 0.1160 - away_acc: 0.1176 - player_acc: 0.0497 - val_loss: 208.7582 - val_home_loss: 2.8019 - val_away_loss: 2.7729 - val_player_loss: 4.0637 - val_home_acc: 0.1749 - val_away_acc: 0.1804 - val_player_acc: 0.0887\n",
      "Epoch 4/6\n",
      "30003/30003 [==============================] - 19s 648us/step - loss: 214.0139 - home_loss: 2.7880 - away_loss: 2.7695 - player_loss: 4.1691 - home_acc: 0.1484 - away_acc: 0.1455 - player_acc: 0.0694 - val_loss: 197.2690 - val_home_loss: 2.6821 - val_away_loss: 2.6553 - val_player_loss: 3.8386 - val_home_acc: 0.2088 - val_away_acc: 0.2265 - val_player_acc: 0.1162\n",
      "Epoch 5/6\n",
      "30003/30003 [==============================] - 20s 670us/step - loss: 205.8646 - home_loss: 2.7271 - away_loss: 2.7012 - player_loss: 4.0087 - home_acc: 0.1696 - away_acc: 0.1648 - player_acc: 0.0855 - val_loss: 194.0448 - val_home_loss: 2.6534 - val_away_loss: 2.6228 - val_player_loss: 3.7754 - val_home_acc: 0.2237 - val_away_acc: 0.2321 - val_player_acc: 0.1267\n",
      "Epoch 6/6\n",
      "30003/30003 [==============================] - 20s 650us/step - loss: 203.5520 - home_loss: 2.7044 - away_loss: 2.6793 - player_loss: 3.9634 - home_acc: 0.1721 - away_acc: 0.1745 - player_acc: 0.0935 - val_loss: 191.9484 - val_home_loss: 2.6226 - val_away_loss: 2.5917 - val_player_loss: 3.7347 - val_home_acc: 0.2314 - val_away_acc: 0.2470 - val_player_acc: 0.1324\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "model_count = 1\n",
    "\n",
    "def schedule(epoch, lr_):\n",
    "    if epoch == 4:\n",
    "        return lr_ * 0.2\n",
    "    return lr_\n",
    "    \n",
    "for splitstrain, splitstest in skf.split(np.zeros(len(df)), df.player.tolist()):\n",
    "    print(\"MODEL:\", model_count)\n",
    "\n",
    "    home_train, away_train, player_train = X_HOME[splitstrain], X_AWAY[splitstrain], X_PLAYER[splitstrain]\n",
    "    actions_train, actions_teams_train = X_ACTIONS[splitstrain], X_ACTIONS_TEAMS[splitstrain]\n",
    "    bf_ctxs_train, bf_teams_train = X_BEFORE_CTXS[splitstrain], X_BEFORE_TEAMS[splitstrain]\n",
    "    af_ctxs_train, af_teams_train = X_AFTER_CTXS[splitstrain], X_AFTER_TEAMS[splitstrain]\n",
    "    timings_train = X_TIMINGS[splitstrain]\n",
    "    y_train = Y[splitstrain]\n",
    "    y_home_train = y_home[splitstrain]\n",
    "    y_away_train = y_away[splitstrain]\n",
    "\n",
    "\n",
    "    home_test, away_test, player_test = X_HOME[splitstest], X_AWAY[splitstest], X_PLAYER[splitstest]\n",
    "    actions_test, actions_teams_test = X_ACTIONS[splitstest], X_ACTIONS_TEAMS[splitstest]\n",
    "    bf_ctxs_test, bf_teams_test = X_BEFORE_CTXS[splitstest], X_BEFORE_TEAMS[splitstest]\n",
    "    af_ctxs_test, af_teams_test = X_AFTER_CTXS[splitstest], X_AFTER_TEAMS[splitstest]\n",
    "    timings_test = X_TIMINGS[splitstest]\n",
    "    y_test = Y[splitstest]\n",
    "    y_home_test = y_home[splitstest]\n",
    "    y_away_test = y_away[splitstest]\n",
    "\n",
    "    K.clear_session()\n",
    "    callback_list = [LearningRateScheduler(schedule, verbose = 0)]\n",
    "\n",
    "    model = build_model()\n",
    "    model.layers[24].layer.layers[7].trainable = True\n",
    "    print(model.layers[24].layer.layers[7].name, model.layers[24].layer.layers[7].trainable)\n",
    "    model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
    "                  loss_weights=[1, 1, 50],\n",
    "                optimizer=Adam(lr = 0.0005),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit([home_train, away_train, player_train,\n",
    "                  actions_train, actions_teams_train,\n",
    "                  bf_ctxs_train, bf_teams_train,\n",
    "                  af_ctxs_train, af_teams_train, timings_train],\n",
    "                 [y_home_train, y_away_train, y_train],\n",
    "                 validation_data=([home_test, away_test, player_test,\n",
    "                                   actions_test, actions_teams_test,\n",
    "                                   bf_ctxs_test, bf_teams_test,\n",
    "                                   af_ctxs_test, af_teams_test, timings_test],\n",
    "                                  [y_home_test, y_away_test, y_test]),\n",
    "                 epochs=6,\n",
    "                 callbacks=callback_list,\n",
    "                 verbose=1,\n",
    "                 batch_size=128\n",
    "                )\n",
    "    \n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(\"next_event_model_%d.json\" % model_count, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"next_event_model_%d.json\" % model_count)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    model_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "full_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
